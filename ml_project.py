# -*- coding: utf-8 -*-
"""ML Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1glBNh3Dl6JuF-RwNunlEYA1udJlLSOJi
"""

!pip install jetnet
import numpy as np
import torch
from sklearn.metrics import confusion_matrix
import seaborn as sns
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
from torch.utils.data import sampler
import torchvision.datasets as datasets
import torchvision.transforms as T
import torchvision.models as models
import matplotlib.pyplot as plt
import os
import random
from jetnet.datasets import JetNet

from jetnet.datasets.normalisations import FeaturewiseLinear
from sklearn.preprocessing import OneHotEncoder
import seaborn as sns

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Jets/
# %ls
!python --version

# En otra celda de Jupyter Notebook:
import os

PROJECT_PATH = '/content/drive/MyDrive/Jets'
MODELS_PATH = f'{PROJECT_PATH}/VIT'
os.makedirs(MODELS_PATH, exist_ok=True)

random.seed(42)

DATA_PATH = f'{PROJECT_PATH}/Datasets/JetNet'
os.makedirs(DATA_PATH, exist_ok=True)

data_args = {
    "jet_type": ['g', 'q'],    #'q', 'z', 't', 'w'],
    "data_dir": DATA_PATH,
    "particle_features": ["etarel", "phirel", "ptrel"],
    "num_particles": 30,
    "jet_features": ["type", "pt", "eta", "mass"],
    #"num_jets": 40000,
    "download": True
}
particle_data, jet_data = JetNet.getData(**data_args)
particle_data = particle_data[:40000]
jet_data = jet_data[:40000]


print(f"New particle data shape: {particle_data.shape}")

import numpy as np

# Aggregate particle data: Compute the mean of particle features for each jet
aggregated_particle_data = np.mean(particle_data, axis=1)  # Shape: (80000, 3)

# Combine aggregated particle data with jet data
combined_data = np.concatenate((aggregated_particle_data, jet_data), axis=1)  # Shape: (80000, 7)

print(f"Combined data shape: {combined_data.shape}")

from sklearn.model_selection import train_test_split

X=combined_data[:,:-1] #All columns except the last one
y=combined_data[:,-1] #Only the last column

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}")

"""Step 1: Feature Extraction"""

# Apply Fisher Discriminant Analysis for feature projection
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder #Import LabelEncoder
from sklearn.model_selection import train_test_split # Import train_test_split
import numpy as np

# Split data into training and testing sets before using y_train and y_test
X=combined_data[:,:-1] #All columns except the last one
y=combined_data[:,-1] #Only the last column
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lda = LinearDiscriminantAnalysis(n_components=None)

# Create a LabelEncoder object
le = LabelEncoder()

# Fit the LabelEncoder to all unique values in both y_train and y_test
# Get all unique values from both arrays
all_unique_values = np.unique(np.concatenate((y_train, y_test)))
# Fit the LabelEncoder to all unique values
le.fit(all_unique_values)

# Transform y_train and y_test using the fitted encoder
y_train_encoded = le.transform(y_train)
y_test_encoded = le.transform(y_test)
globals()['y_test_encoded'] = y_test_encoded

# Now using the encoded target variable for training
X_train_fda = lda.fit_transform(X_train, y_train_encoded)
X_test_fda = lda.transform(X_test)

# Train a classifier on the FDA features
clf_fda = RandomForestClassifier(random_state=42)
clf_fda.fit(X_train_fda, y_train_encoded)  # Use encoded y_train
y_pred_fda = clf_fda.predict(X_test_fda)

globals()['y_pred_fda'] = y_pred_fda

# Evaluating the performance
from sklearn.metrics import accuracy_score, classification_report

accuracy_fda = accuracy_score(y_test_encoded, y_pred_fda)  # Use encoded y_test
print(f"FDA Feature Extraction - Accuracy: {accuracy_fda}")
print("Classification Report (FDA):")
print(classification_report(y_test_encoded, y_pred_fda)) #Use encoded y_test

# Apply ReliefF for feature selection
relief = ReliefF(n_neighbors=10, n_features_to_select=10)  # Select top 10 features
X_train_relief = relief.fit_transform(X_train, y_train)
X_test_relief = relief.transform(X_test)

# Train a classifier on the ReliefF features
clf_relief = RandomForestClassifier(random_state=42)
clf_relief.fit(X_train_relief, y_train)
y_pred_relief = clf_relief.predict(X_test_relief)

# Evaluate the performance
accuracy_relief = accuracy_score(y_test, y_pred_relief)
print(f"ReliefF Feature Selection - Accuracy: {accuracy_relief}")
print("Classification Report (ReliefF):")
print(classification_report(y_test, y_pred_relief))

print("Comparison of Dimensionality Reduction Methods:")
print(f"Accuracy with FDA: {accuracy_fda}")
print(f"Accuracy with ReliefF: {accuracy_relief}")

"""Step 2: Classifier Implementation"""

from scipy.spatial import distance
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix, accuracy_score

# Mahalanobis Distance Classifier
class MahalanobisClassifier:
    def fit(self, X, y):
        self.means_ = {}
        self.cov_matrix_ = np
        classes = np.unique(y)
        for cls in classes:
            self.means_[cls] = X[y == cls].mean(axis=0)

    def predict(self, X):
        predictions = []
        for x in X:
            distances = {cls: distance.mahalanobis(x, mean, np.linalg.inv(self.cov_matrix_)) for cls, mean in self.means_.items()}
            predictions.append(min(distances, key=distances.get))
        return np.array(predictions)

# Train and evaluate
mahalanobis = MahalanobisClassifier()
y_pred_mahalanobis = cross_val_predict(mahalanobis, X, y, cv=5)
print(confusion_matrix(y, y_pred_mahalanobis))

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

# KNN with GridSearchCV for best K
param_grid = {'n_neighbors': range(1, 21)}
knn = KNeighborsClassifier()
grid = GridSearchCV(knn, param_grid, cv=5)
grid.fit(X_train, y_train_encoded)

# Best K value
best_k = grid.best_params_['n_neighbors']
print(f"Best K: {best_k}")

# Evaluate KNN
knn = KNeighborsClassifier(n_neighbors=best_k)
y_pred_knn = cross_val_predict(knn, X, y, cv=5)
print(confusion_matrix(y, y_pred_knn))

from sklearn.cluster import KMeans

# K-Means Classifier
kmeans = KMeans(n_clusters=len(np.unique(y)))
kmeans.fit(X_train)
y_pred_kmeans = kmeans.predict(X_test)

# Confusion Matrix
print(confusion_matrix(y_test_encoded, y_pred_kmeans))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Build Neural Network
nn = Sequential([
    Dense(64, input_dim=X_train.shape[1], activation='relu'),
    Dense(32, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')  # Output layer for classification
])
nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train and evaluate
nn.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test, y_test_encoded))
y_pred_nn = nn.predict(X_test)
print(confusion_matrix(y_test_encoded, np.argmax(y_pred_nn, axis=1)))

from sklearn.svm import SVC

# SVM Classifier
svm = SVC(kernel='linear')
y_pred_svm = cross_val_predict(svm, X, y, cv=5)

# Confusion Matrix
print(confusion_matrix(y, y_pred_svm))

from sklearn.ensemble import RandomForestClassifier

# Random Forest Classifier
rf = RandomForestClassifier(random_state=42)
y_pred_rf = cross_val_predict(rf, X, y, cv=5)

# Confusion Matrix
print(confusion_matrix(y, y_pred_rf))

"""Step 3: Visualization of Confusion Matrices"""

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(title)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

plot_confusion_matrix(y, y_pred_mahalanobis, "Mahalanobis Distance")
plot_confusion_matrix(y, y_pred_knn, "KNN")
plot_confusion_matrix(y_test_encoded, y_pred_kmeans, "K-Means")
plot_confusion_matrix(y_test_encoded, np.argmax(y_pred_nn, axis=1), "Neural Network")
plot_confusion_matrix(y, y_pred_svm, "SVM")
plot_confusion_matrix(y, y_pred_rf, "Random Forest")